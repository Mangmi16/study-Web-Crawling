{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_12812/1851421343.py:28: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_12812/1851421343.py:36: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  element = driver.find_element_by_id(\"inp_search\")\n"
     ]
    }
   ],
   "source": [
    "#Step 1. 필요한 모듈과 라이브러리를 import 합니다\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import numpy  \n",
    "import pandas as pd  \n",
    "import xlwt\n",
    "\n",
    "#Step 2. 필요한 정보를 입력 받습니다\n",
    "# query_txt = input('1.크롤링할 키워드는 무엇입니까?: ')\n",
    "query_txt= '겨울'\n",
    "\n",
    "# cnt = int(input('2.크롤링 할 건수는 몇건입니까?: '))\n",
    "cnt = 12\n",
    "page_cnt = math.ceil(cnt / 10)  # 크롤링 할 전체 페이지 수 \n",
    "\n",
    "# f_name = input('3.txt 형태로 저장할 경로와 파일명을 확장자 포함해서 쓰세요: ')\n",
    "# fc_name = input('4.csv 형태로 저장할 경로와 파일명을 확장자 포함해서 쓰세요: ')\n",
    "# fx_name = input('5.xls 형태로 저장할 경로와 파일명을 확장자 포함해서 쓰세요: ')\n",
    "\n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "s_time = time.time( )\n",
    "path = \"c:/temp/chromedriver_94/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "driver.get('https://korean.visitkorea.or.kr/main/main.html')\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 4. 검색창의 이름을 찾아서 검색어를 입력 후 검색을 진행합니다\n",
    "# driver.find_element_by_id(\"btnSearch\").click()\n",
    "element = driver.find_element_by_id(\"inp_search\")\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys('\\n')\n",
    "\n",
    "#학습 목표 2: 페이지 이동하기\n",
    "#Step 6. 페이지 번호를 바꾸면서 실제 데이터 추출하기\n",
    "\n",
    "no = 1            # 게시글 번호 카운트용 변수\n",
    "\n",
    "no_list =[ ]          # 게시글 번호 저장용 리스트\n",
    "title_li=[ ]     # 게시글 내용 저장용 리스트\n",
    "region_li=[ ]         # 지역 정보 저장용 리스트\n",
    "hashtag_li=[ ]         # 해쉬 태그 정보 저장용 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #요약 내용을 가져오는 함수\n",
    "def summary(no=no, \n",
    "            no_list = no_list,\n",
    "            title_li=title_li,\n",
    "            region_li=region_li,\n",
    "            hashtag_li =hashtag_li):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content_list = soup.find('ul','list_thumType type1').find_all('div','area_txt')\n",
    "\n",
    "    for i in content_list:\n",
    "        try:\n",
    "            title = i.find('div', 'tit').text.strip()\n",
    "        except:\n",
    "            print(\"내용이 없습니다\")\n",
    "            continue\n",
    "        else:\n",
    "            region = i.find('div', 'service').text.strip()\n",
    "            hashtag = i.find('p', 'tag_type').text.strip()\n",
    "\n",
    "            print(\"article no : \", no)\n",
    "            print(\"제목 : \", title)\n",
    "            print(\"지역 : \", region)        \n",
    "            print(\"해쉬태그 : \", hashtag)\n",
    "            print(\"=-\"*60)\n",
    "\n",
    "            no_list.append(no)\n",
    "            title_li.append(title)\n",
    "            region_li.append(region)\n",
    "            hashtag_li.append(hashtag)\n",
    "            no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상세 페이지 크롤링\n",
    "# 가져올 데이터 : 제목, 내용, 수정일, 좋아요, 조회수\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get('https://korean.visitkorea.or.kr/detail/cs_detail_cos.do?cotid=b1aae4c5-e3dc-4a6b-9ef3-fe017e40344a&big_category=C01&mid_category=C0112&big_area=6')\n",
    "driver.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_li_idv = []\n",
    "contents_li_idv = []\n",
    "date_li_idv = []\n",
    "likes_li_idv = []\n",
    "views_li_idv = []\n",
    "region_li_idv = []\n",
    "hashtag_li_idv = []\n",
    "\n",
    "global no\n",
    "no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제4 : 상세 페이지 정보 긁어오기\n",
    "def single_page (no):\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    head = soup.find('div', class_=\"tit_cont titleType1\")\n",
    "    body = soup.find('div', class_=\"box_txtPhoto\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        title = head.find('h2').text.strip()\n",
    "    except:\n",
    "        print(\"내용이 없습니다\")\n",
    "    else:\n",
    "        region_date = head.find('div','area_address').find_all('span')\n",
    "        region = region_date[0].text.strip()\n",
    "        date = region_date[1].text.strip()\n",
    "        likes = head.find('span',id=\"conLike\").text.strip()\n",
    "        views = head.find('span', id=\"conRead\").text.strip()\n",
    "        body = soup.find('div', class_=\"box_txtPhoto\").text.strip()\n",
    "\n",
    "\n",
    "        print(\"article no : \", no)\n",
    "        print(\"제목 : \", title)\n",
    "        print(\"지역 : \", region)\n",
    "        print(\"날짜 : \", date)\n",
    "        print(\"좋아요 : \", likes)\n",
    "        print(\"조회수 : \", views)\n",
    "        print(\"해쉬태그 : \", hashtag)\n",
    "        print(\"본문 : \", body)\n",
    "        print(\"=-\"*60)\n",
    "\n",
    "        title_li_idv.append(title)\n",
    "        contents_li_idv.append(body)\n",
    "        date_li_idv.append(date)\n",
    "        likes_li_idv.append(likes)\n",
    "        views_li_idv.append(views)\n",
    "        #지역\n",
    "        region_li_idv.append(region)\n",
    "        #해쉬태그\n",
    "        hashtag_li_idv.append(hashtag)\n",
    "        \n",
    "        no+=1\n",
    "\n",
    "single_page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_12812/3656374695.py:5: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_css_selector('#listBody > ul > li:nth-child(%s) > div.area_txt > div.tit > a' %title_no).click( )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 문제 5 : 상세 페이지 하나씩 들어갔다 나오기\n",
    "for title_no in range(1,12):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "#     driver.find_element_by_xpath(\"\"\"//*[@id=\"listBody\"]/ul/li[%s]/div[2]/div[1]/a\"\"\" % title_no).click()\n",
    "        driver.find_element_by_css_selector('#listBody > ul > li:nth-child(%s) > div.area_txt > div.tit > a' %title_no).click( )\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        continue\n",
    "    driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,page_cnt+1) :  # 1페이지부터 반복 시작    \n",
    "    print(\"%s 페이지 내용 수집 시작합니다 =======================\" %x)\n",
    "    print(\"\\n\")    \n",
    "    time.sleep(2)              #페이지가 로딩 될 때까지 2초 대기\n",
    "    summary()\n",
    "    x += 1\n",
    "    if x == 6 :\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"6\"]\"\"\").click()\n",
    "    else :\n",
    "        driver.find_element_by_link_text(\"\"\"%s\"\"\" %x).click()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장한 리스트를 csv 파일로 저장하기\n",
    "# csv  \n",
    "1. 리스트 형태로 데이터를 모아준다.\n",
    "2. 리스트를 합해서 DataFrame으로 만들어 준다\n",
    "3. DataFrame을 csv로( ~로 = to) 내보낸다.\n",
    "\n",
    "## 데이터 프레임 만들기\n",
    "데이프레임_이름 = pd.DataFrame() <- 선언을 해 준다\n",
    "\n",
    "데이프레임_이름['행이름1'] = 리스트1\n",
    "\n",
    "...\n",
    "\n",
    "데이프레임_이름['행이름n'] = 리스트n\n",
    "\n",
    "# DataFrame을 csv로( ~로 = to) 내보낸다\n",
    "데이프레임_이름.to_csv(경로, 인덱스=True, encoding=\"utf-8-sig\")\n",
    "+ 엑셀로도 저장해 보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #list 선언\n",
    "# title_list = []\n",
    "# contents_list = []\n",
    "# date_list  = []\n",
    "# press_list = []\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['제목'] = title_list\n",
    "df['내용'] = contents_list\n",
    "df['개시일'] = date_list\n",
    "df['언론사'] = press_list\n",
    "\n",
    "# f_name = \"c:\\\\temp\\\\test.csv\"\n",
    "f_name_xls = \"c:\\\\temp\\\\test.xls\"\n",
    "# df.to_csv(f_name, encoding = 'utf-8-sig')\n",
    "df.to_excel(f_name_xls, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
